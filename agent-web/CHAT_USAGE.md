# 智能交互聊天功能使用说明

## 功能特性

### 1. 真实LLM集成
- 支持与配置的LLM模型进行真实对话
- 默认支持 DeepSeek-Chat、DeepSeek-Reasoner 和 Qwen3-235B-A22B 模型
- 根据设置中配置的模型自动切换

### 2. 流式响应
- 支持流式响应，实时显示AI的回复内容
- 显示加载指示器，提供更好的用户体验
- 自动滚动到最新消息

### 3. Thinking模型支持
- 对于thinking模型（如DeepSeek-Reasoner和Qwen3-235B-A22B），会专门显示思考过程
- 思考过程显示在独立的灰色框中，可以折叠显示
- 思考内容完全来源于模型的真实思考过程，不影响主要对话内容
- 非thinking模型（如DeepSeek-Chat）不会显示思考框

### 4. 聊天记忆
- 保持完整的对话历史
- 每次发送消息时都会包含之前的对话上下文
- AI能够理解和引用之前的对话内容

### 5. 错误处理与重试
- 当网络错误或API调用失败时，显示错误消息
- 提供重试按钮，可以重新发送最后一条消息
- 流式响应失败时，自动尝试同步方式获取回复

## 使用方法

### 1. 配置LLM模型
1. 点击右上角的"设置"按钮
2. 在LLM配置中选择要使用的模型
3. 验证API配置信息是否正确
4. 点击"确定"保存配置

### 2. 开始聊天
1. 在右侧交互区的输入框中输入消息
2. 按回车键或点击发送按钮
3. 等待AI回复（会显示加载状态）
4. 查看AI的回复和思考过程（如果是thinking模型）

### 3. 查看思考过程
- 对于thinking模型，AI的思考过程会显示在专门的灰色框中
- 思考过程标有"思考过程"标题和闪电图标
- 可以展开或折叠查看详细的思考内容

### 4. 错误处理
- 如果出现错误，消息会显示为红色
- 点击"重试"按钮可以重新发送消息
- 检查网络连接和API配置是否正确

## 技术实现

### API调用格式
```javascript
{
  model: "deepseek-chat", // 或 "qwen3-235b-a22b"
  messages: [
    { role: "system", content: "系统提示" },
    { role: "user", content: "用户消息" },
    { role: "assistant", content: "AI回复" }
  ],
  temperature: 0.6,
  stream: true
}
```

### 流式响应处理
- 使用 Server-Sent Events (SSE) 接收流式响应
- 解析 `data:` 格式的消息
- 处理 `delta.content` 和 `delta.reasoning` 字段

### Thinking模型特殊处理
- DeepSeek-Reasoner: 使用 `delta.reasoning` 字段获取思考内容
- Qwen3-235B-A22B: 从 `delta.content` 中解析 `<thinking>` 标签
- DeepSeek-Chat: 普通对话模型，不显示思考过程

## 故障排除

### 常见问题
1. **API Key错误**: 检查设置中的API Key是否正确
2. **网络连接问题**: 确认网络连接正常，API地址可访问
3. **模型不支持**: 确认选择的模型ID在API服务商处可用
4. **流式响应失败**: 系统会自动尝试同步方式获取回复

### 调试信息
- 打开浏览器开发者工具查看控制台日志
- 网络请求失败会在控制台显示详细错误信息
- 可以查看具体的API调用参数和响应内容 