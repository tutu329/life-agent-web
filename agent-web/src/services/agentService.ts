// Agentç³»ç»Ÿç›¸å…³çš„æ•°æ®ç±»å‹å®šä¹‰
export interface LLM_Config {
  base_url: string
  api_key: string
  llm_model_id: string
  temperature: number
  top_p: number
  max_new_tokens: number
  reasoning_effort: string
  vpn_on: boolean
  chatml: boolean
}

export interface Agent_Config {
  tool_names: string[]
  exp_json_path: string
  llm_config: LLM_Config
}

export interface Agent_As_Tool_Config {
  tool_names: string[]
  exp_json_path: string
  as_tool_name: string
  as_tool_description: string
  llm_config: LLM_Config
}

export interface Agents_System_Request {
  remote_tools: any[]
  upper_agent_config: Agent_Config
  lower_agents_config: Agent_As_Tool_Config[]
}

export interface Query_Agent_Context {
  custom_data_dict: { [key: string]: any }
}

export interface Query_Agent_Request {
  agent_id: string
  query: string
  context: Query_Agent_Context
}

export interface Agent_Status_Request {
  agent_id: string
}

export interface StreamEvent {
  data: string
  type?: string
  event?: string
}

export interface StreamResponse {
  id: string
  streams: string[]
}

export type StreamType = 'output' | 'thinking' | 'final_answer' | 'log' | 'tool_rtn_data'

export class SSEClient {
  private reader: ReadableStreamDefaultReader<Uint8Array>
  private decoder: TextDecoder

  constructor(response: Response) {
    this.reader = response.body!.getReader()
    this.decoder = new TextDecoder()
  }

  async* events(): AsyncGenerator<StreamEvent> {
    let buffer = ''
    
    while (true) {
      const { done, value } = await this.reader.read()
      
      if (done) break
      
      buffer += this.decoder.decode(value, { stream: true })
      
      const lines = buffer.split('\n')
      buffer = lines.pop() || ''
      
      let event: Partial<StreamEvent> = {}
      
      for (const line of lines) {
        const trimmedLine = line.trim()
        
        if (trimmedLine === '') {
          if (event.data !== undefined) {
            yield event as StreamEvent
            event = {}
          }
        } else if (trimmedLine.startsWith('data: ')) {
          event.data = trimmedLine.slice(6)
        } else if (trimmedLine.startsWith('event: ')) {
          event.event = trimmedLine.slice(7)
        } else if (trimmedLine.startsWith('id: ')) {
          // å¤„ç†idå­—æ®µï¼ˆå¦‚æœéœ€è¦ï¼‰
        }
      }
    }
  }
}

export class AgentService {
  private baseUrl: string
  private agentId: string | null = null

  constructor(serverName: string = 'powerai.cc') {
    this.baseUrl = `https://${serverName}:5110`
  }

  // åˆå§‹åŒ–Agentç³»ç»Ÿ
  async initializeAgentSystem(): Promise<string> {
    // è¾“å‡ºç¯å¢ƒå˜é‡è°ƒè¯•ä¿¡æ¯
    console.log('ğŸ”‘ VITE_GROQ_API_KEY ç¯å¢ƒå˜é‡:', import.meta.env.VITE_GROQ_API_KEY || 'æœªè®¾ç½®')
    console.log('ğŸ”‘ ç¯å¢ƒå˜é‡é•¿åº¦:', (import.meta.env.VITE_GROQ_API_KEY || '').length)

    const request: Agents_System_Request = {
      remote_tools: [],
      upper_agent_config: {
        tool_names: [],
        //tool_names: ['Folder_Tool'],
        exp_json_path: 'my_2_levels_mas_exp.json',
        llm_config: {
          base_url: 'http://powerai.cc:8001/v1',
          api_key: 'empty',
          llm_model_id: 'openai/gpt-oss-120b',
          temperature: 1.0,
          top_p: 1.0,
          max_new_tokens: 4096,
          reasoning_effort: 'low',
          vpn_on: false,
          chatml: true
        }
        // llm_config: {
        //   base_url: 'https://api.deepseek.com/v1',
        //   api_key: 'sk-c1d34a4f21e3413487bb4b2806f6c4b8',
        //   llm_model_id: 'deepseek-chat',
        //   temperature: 0.65,
        //   top_p: 0.9,
        //   max_new_tokens: 1000,
        //   vpn_on: false
        // }
      },
      lower_agents_config: [
        {
          tool_names: ['Write_Chapter_Tool'],
          // tool_names: ['Human_Console_Tool', 'Folder_Tool'],
          exp_json_path: '',
          as_tool_name: 'Write_Chapter_Agent_As_Tool',
          // as_tool_name: 'Folder_Agent_As_Tool',
          as_tool_description: 'æœ¬å·¥å…·ç”¨äºåœ¨officeæ–‡æ¡£ä¸­ç¼–åˆ¶ä¸€ä¸ªç« èŠ‚çš„å†…å®¹',
          llm_config: {
            base_url: 'http://powerai.cc:8001/v1',
            api_key: 'empty',
            llm_model_id: 'openai/gpt-oss-120b',
            temperature: 1.0,
            top_p: 1.0,
            max_new_tokens: 4096,
            reasoning_effort: 'low',
            vpn_on: false,
            chatml: true
          }
          // llm_config: {
          //   base_url: 'https://api.deepseek.com/v1',
          //   api_key: 'sk-c1d34a4f21e3413487bb4b2806f6c4b8',
          //   llm_model_id: 'deepseek-chat',
          //   temperature: 0.70,
          //   top_p: 0.9,
          //   max_new_tokens: 1000,
          //   vpn_on: false
          // }
        }
      ]
    }

    // const request: Agents_System_Request = {
    //   remote_tools: [],
    //   upper_agent_config: {
    //     tool_names: ['Folder_Tool'],
    //     exp_json_path: 'my_2_levels_mas_exp.json',
    //     llm_config: {
    //       base_url: 'https://api.groq.com/openai/v1',
    //       api_key: import.meta.env.VITE_GROQ_API_KEY || '',
    //       llm_model_id: 'moonshotai/kimi-k2-instruct',
    //       temperature: 0.6,
    //       top_p: 0.9,
    //       max_new_tokens: 8192,
    //       vpn_on: true
    //     }
    //   },
    //   lower_agents_config: [
    //     {
    //       tool_names: ['Write_Chapter_Tool'],
    //       // tool_names: ['Human_Console_Tool', 'Folder_Tool'],
    //       exp_json_path: '',
    //       as_tool_name: 'Write_Chapter_Agent_As_Tool',
    //       // as_tool_name: 'Folder_Agent_As_Tool',
    //       as_tool_description: 'æœ¬å·¥å…·ç”¨äºåœ¨officeæ–‡æ¡£ä¸­ç¼–åˆ¶ä¸€ä¸ªç« èŠ‚çš„å†…å®¹',
    //       llm_config: {
    //         base_url: 'https://api.groq.com/openai/v1',
    //         api_key: import.meta.env.VITE_GROQ_API_KEY || '',
    //         llm_model_id: 'moonshotai/kimi-k2-instruct',
    //         temperature: 0.6,
    //         top_p: 0.9,
    //         max_new_tokens: 8192,
    //         vpn_on: true
    //       }
    //     }
    //   ]
    // }

    try {
      const response = await fetch(`${this.baseUrl}/api/start_2_level_agents_system`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(request),
      })

      if (response.ok) {
        const result = await response.json()
        const agentId = typeof result === 'string' ? result : result.toString()
        this.agentId = agentId
        console.log('âœ… Agentç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼Œagent_id:', this.agentId)
        return agentId
      } else {
        const errorText = await response.text()
        throw new Error(`Agentç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: ${response.status} - ${errorText}`)
      }
    } catch (error) {
      console.error('âŒ Agentç³»ç»Ÿåˆå§‹åŒ–é”™è¯¯:', error)
      throw error
    }
  }

  // æŸ¥è¯¢Agentç³»ç»Ÿ
  async queryAgentSystem(query: string, context?: Query_Agent_Context): Promise<StreamResponse> {
    if (!this.agentId) {
      throw new Error('Agentç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initializeAgentSystem()')
    }

    const request: Query_Agent_Request = {
      agent_id: this.agentId,
      query: query,
      context: context || { custom_data_dict: {} }
    }

    try {
      const response = await fetch(`${this.baseUrl}/api/query_2_level_agents_system`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(request),
      })

      if (response.ok) {
        const result = await response.json()
        console.log('âœ… æŸ¥è¯¢Agentç³»ç»ŸæˆåŠŸ:', result)
        return result as StreamResponse
      } else {
        const errorText = await response.text()
        throw new Error(`æŸ¥è¯¢Agentç³»ç»Ÿå¤±è´¥: ${response.status} - ${errorText}`)
      }
    } catch (error) {
      console.error('âŒ æŸ¥è¯¢Agentç³»ç»Ÿé”™è¯¯:', error)
      throw error
    }
  }

  // ç›‘å¬SSEæµ
  async* listenToStream(streamId: string, streamName: string): AsyncGenerator<StreamEvent> {
    const streamUrl = `${this.baseUrl}/api/query_2_level_agents_system/stream/${streamId}/${streamName}`
    
    try {
      const response = await fetch(streamUrl, {
        method: 'GET',
        headers: {
          'Accept': 'text/event-stream',
          'Cache-Control': 'no-cache',
        },
      })

      if (!response.ok) {
        throw new Error(`æµè¿æ¥å¤±è´¥: ${response.status} - ${response.statusText}`)
      }

      const client = new SSEClient(response)
      for await (const event of client.events()) {
        yield event
      }
    } catch (error) {
      console.error(`âŒ æµ ${streamName} å‡ºé”™:`, error)
      throw error
    }
  }

  // æ£€æŸ¥AgentçŠ¶æ€
  async checkAgentStatus(): Promise<{ finished: boolean }> {
    if (!this.agentId) {
      throw new Error('Agentç³»ç»Ÿæœªåˆå§‹åŒ–')
    }

    const request: Agent_Status_Request = {
      agent_id: this.agentId
    }

    try {
      console.log('ğŸš€ å¼€å§‹å‘é€çŠ¶æ€æ£€æŸ¥è¯·æ±‚...')
      const response = await fetch(`${this.baseUrl}/api/get_agent_status`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(request)
      })
      console.log('ğŸ“¡ æ”¶åˆ°å“åº”ï¼ŒçŠ¶æ€ç :', response.status)
      
      if (response.ok) {
        const status = await response.json()
        console.log(`ğŸ“Š AgentçŠ¶æ€: finished=${status.finished}`)
        return status
      } else {
        const errorText = await response.text()
        throw new Error(`æ£€æŸ¥çŠ¶æ€å¤±è´¥: ${response.status} - ${errorText}`)
      }
    } catch (error) {
      console.error('âŒ æ£€æŸ¥AgentçŠ¶æ€é”™è¯¯:', error)
      throw error
    }
  }

  // è·å–å½“å‰agent_id
  getAgentId(): string | null {
    return this.agentId
  }

  // é‡ç½®agent_id
  resetAgentId(): void {
    this.agentId = null
  }
} 